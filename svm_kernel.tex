\chapter{SVM with Kernels}

\begin{multicols*}{2}

\section{Non-linear SVM}

\noindent To generalize linear decision boundary to become non-linear, we transform $\mathbf x_i$ to higher dimensional space using a function $\varphi(\mathbf{x_i})$.

$$\varphi : \mathbf{x} \rightarrow \varphi(\mathbf{x})$$

\noindent Assumption: in a higher dimensional space, it is easier to find a linear hyperplane to classify data. \\

\noindent The optimization problem of non-linear SVM becomes:

$$\!\min_w \frac{\| \mathbf{w} \|_2^2}{2}$$
$$\text{s.t. } y_i \times (\mathbf{w} \cdot \varphi(\mathbf{x}_i) + b) \ge 1$$

\noindent However, computation in the feature space can be costly because it is high dimensional. As a result, we use kernel trick for optimization. 

\section{Kernel Trick}

\noindent If $\varphi(\cdot)$ satisfies Mercer function (positive-definite function), then we can find a kernel function $k(\cdot,\cdot)$ such that:

$$k(\mathbf{x}_i, \mathbf{x}_j) = \varphi(\mathbf{x}_i)\cdot \varphi(\mathbf{x}_j)$$

\subsection{Kernel Function Examples}
\noindent Linear kernel:
$$k(\mathbf{x}_i,\mathbf{x}_j)=\mathbf{x}_i \cdot \mathbf{x}_j$$

\noindent Radial basis function kernel:
$$k(\mathbf{x}_i,\mathbf{x}_j)=exp\Big(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|_2^2}{2 \sigma^2} \Big)$$

\noindent Polynomial kernel:
$$k(\mathbf{x}_i,\mathbf{x}_j)=(\mathbf{x}_i\cdot \mathbf{x}_j + 1)^d$$

\section{Dual Form}

\noindent Using Lagrangian Multiplier, we can transform the original optimization problem to dual form, where the mapping function $\varphi$ does no need to be computed explicitly during numeric optimization:

$$\!\max_{\lambda} \Big( \sum_{i=1}^N \lambda_i - \frac{1}{2} \sum_{i,j} \lambda_i \lambda_j y_i y_j (\varphi(\mathbf{x}_i) \cdot \varphi(\mathbf{x}_j)  \Big)$$

\noindent The decision boundary is given by:

\begin{equation*}
\begin{split}
    \mathbf{w} \cdot \varphi(\mathbf{x}) + b &= 0\\
    \sum_{i=1}^N \lambda_i y_i \varphi(\mathbf{x}_i) \cdot \varphi(\mathbf{x}) + b &= 0
\end{split}
\end{equation*}

\noindent For a test instance $\mathbf{x}^*$, it can be classified using:

$$f(\mathbf{x}^*)= \text{sign} \Bigg( \sum_{i=1}^N \lambda_i y_i \varphi(\mathbf{x}_i) \cdot \varphi(\mathbf{x}^*) + b \Bigg)$$


\end{multicols*}
